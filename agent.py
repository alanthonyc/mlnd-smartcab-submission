import random, sys, math, operator, argparse, os, ast
from environment import Agent, Environment
from planner import RoutePlanner
from simulator import Simulator
import pandas as pd

VERBOSE = False
DEBUG = False
DEFAULT_ALPHA = 0.95
DEFAULT_EPSILON_A = 0.95
DISPLAY = True

def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument("epsilon", nargs="?", default="empty_string")
    parser.add_argument("loops", type=int, nargs="?", default=1)
    return parser.parse_args()

class LearningAgent(Agent):
    """ An agent that learns to drive in the Smartcab world.
        This is the object you will be modifying. """ 

    def __init__(self, env, learning=False, epsilon=1.0, alpha=0.5):
        super(LearningAgent, self).__init__(env)     # Set the agent in the evironment 
        self.planner = RoutePlanner(self.env, self)  # Create a route planner
        self.valid_actions = self.env.valid_actions  # The set of valid actions

        # Set parameters of the learning agent
        self.learning = learning # Whether the agent is expected to learn
        self.Q = dict()          # Create a Q-table which will be a dictionary of tuples
        self.epsilon = epsilon   # Random exploration factor
        self.alpha = alpha       # Learning factor

        ###########
        ## TO DO ##
        # TODO  1
        ###########
        # Set any additional class parameters as needed
        self.trials = 0
        self.epsilonA = DEFAULT_EPSILON_A


    def reset(self, destination=None, testing=False):
        """ The reset function is called at the beginning of each trial.
            'testing' is set to True if testing trials are being used
            once training trials have completed. """

        # Select the destination as the new location to route to
        self.planner.route_to(destination)
        
        ########### 
        ## TO DO ##
        # TODO  2
        ###########
        # Update epsilon using a decay function of your choice
        # Update additional class parameters as needed
        # If 'testing' is True, set epsilon and alpha to 0
        self.trials += 1
        if testing:
            self.epsilon = 0
            self.alpha = 0
        else:
            self.epsilon = self.epsilonA ** self.trials

        return None

    def build_state(self):
        """ The build_state function is called when the agent requests data from the 
            environment. The next waypoint, the intersection inputs, and the deadline 
            are all features available to the agent. """

        # Collect data about the environment
        waypoint = self.planner.next_waypoint() # The next waypoint 
        inputs = self.env.sense(self)           # Visual input - intersection light and traffic
        deadline = self.env.get_deadline(self)  # Remaining deadline

        ########### 
        ## TO DO ##
        # TODO  3
        ###########
        
        # NOTE : you are not allowed to engineer features outside of the inputs available.
        # Because the aim of this project is to teach Reinforcement Learning, we have placed 
        # constraints in order for you to learn how to adjust epsilon and alpha, and thus learn about the balance between exploration and exploitation.
        # With the hand-engineered features, this learning process gets entirely negated.
        
        # Set 'state' as a tuple of relevant data for the agent        
        state = (waypoint, inputs['oncoming'], inputs['left'], inputs['light'])
        return state

    def state_key(self, state):
        """
        Return state key (string format) for a given state.
        """
        return state[0], state[1], state[2], state[3]

    def maxQ_index(self, state):
        """
        Return index of action with largest Q for a given state. Randomly select one key, if
        multiple have the same max value.
        """
        key = self.state_key(state)
        max_value = max(self.Q[key].values())
        maxQs = [k for k, v in self.Q[key].items() if v == max_value]
        maxQ_random = random.choice(maxQs)
        return maxQ_random


    def get_maxQ(self, state):
        """ The get_max_Q function is called when the agent is asked to find the
            maximum Q-value of all actions based on the 'state' the smartcab is in. """

        ########### 
        ## TO DO ##
        # TODO  4
        ###########
        # Calculate the maximum Q-value of all actions for a given state
        key = self.state_key(state)
        maxQ_index = self.maxQ_index(state)
        maxQ = self.Q[key][maxQ_index] 
        return maxQ 


    def createQ(self, state):
        """ The createQ function is called when a state is generated by the agent. """

        ########### 
        ## TO DO ##
        # TODO  5
        ###########
        # When learning, check if the 'state' is not in the Q-table
        # If it is not, create a new dictionary for that state
        #   Then, for each action available, set the initial Q-value to 0.0
        key = self.state_key(state)
        if self.learning:
            if key not in self.Q:
                self.Q[key] = dict()
                self.Q[key]['forward'] = 0.0
                self.Q[key]['left'] = 0.0
                self.Q[key]['right'] = 0.0
                self.Q[key][None] = 0.0
                if DEBUG:
                    print "Agent: Creating State"
            elif DEBUG:
                print "Agent: State Found"
        return


    def choose_action(self, state):
        """ The choose_action function is called when the agent is asked to choose
            which action to take, based on the 'state' the smartcab is in. """

        # Set the agent state and default action
        self.state = state
        self.next_waypoint = self.planner.next_waypoint()

        ########### 
        ## TO DO ##
        # TODO 6
        ###########
        # When not learning, choose a random action
        # When learning, choose a random action with 'epsilon' probability
        # Otherwise, choose an action with the highest Q-value for the current state
        # Be sure that when choosing an action with highest Q-value that you randomly select between actions that "tie".
        if self.learning:
            eps = random.random()
            if eps < self.epsilon:
                action = random.choice(self.env.valid_actions)
            else:
                action = self.maxQ_index(state)
        else:
            action = random.choice(self.env.valid_actions)

        return action


    def learn(self, state, action, reward):
        """ The learn function is called after the agent completes an action and
            receives a reward. This function does not consider future rewards 
            when conducting learning. """

        ########### 
        ## TO DO ##
        # TODO 7
        ###########
        # When learning, implement the value iteration update rule
        #   Use only the learning rate 'alpha' (do not use the discount factor 'gamma')
        if self.learning:
            key = self.state_key(state)
            maxQ = self.get_maxQ(state)
            stateQ = self.Q[key][action]
            newQ = (reward * self.alpha) + (stateQ * (1 - self.alpha))
            self.Q[key][action] = newQ
    
            if DEBUG:
                print "===================="
                print "Learning Function:"
                print "Waypoint: %s" % state[0]
                print "----"
                print "          %s" % state[1]
                print "          __"
                print "    %s |" % state[2]
                print "          __"
                print "          %s" % state[3]
                print "----"
                print "Action: %s" % action
                print "===================="
                print "Learning Reward: %f" % reward
                print "Max Q for action: %s" % maxQ
                print "Old Q for state: %f" % stateQ
                print "New Q for state/action: %f" % newQ
                print "===================="

        return


    def update(self):
        """ The update function is called when a time step is completed in the 
            environment for a given trial. This function will build the agent
            state, choose an action, receive a reward, and learn if enabled. """
          
        state = self.build_state()          # Get current state
        self.createQ(state)                 # Create 'state' in Q-table
        action = self.choose_action(state)  # Choose an action
        reward = self.env.act(self, action) # Receive a reward
        self.learn(state, action, reward)   # Q-learn
        return

class LearningAgentEpsilon0(LearningAgent):
    def reset(self, destination=None, testing=False):
        """ 
        Subclass of `LearningAgent` with epsilon function:
        
            `self.alpha ** self.trials`
        """
        self.planner.route_to(destination)
        self.trials += 1
        if testing:
            self.epsilon = 0
            self.alpha = 0
        else:
            self.epsilon = self.epsilon - 0.05
        return None


class LearningAgentEpsilon1(LearningAgent):
    def reset(self, destination=None, testing=False):
        """ 
        Subclass of `LearningAgent` with epsilon function:
        
            `self.alpha ** self.trials`
        """
        self.planner.route_to(destination)
        self.trials += 1
        if testing:
            self.epsilon = 0
            self.alpha = 0
        else:
            self.epsilon = self.epsilonA ** self.trials
        return None

class LearningAgentEpsilon2(LearningAgent):
    def reset(self, destination=None, testing=False):
        """ 
        Subclass of `LearningAgent` with epsilon function:
         
            `1 / self.trials^2`
        """
        self.planner.route_to(destination)
        self.trials += 1
        if testing:
            self.epsilon = 0
            self.alpha = 0
        else:
            self.epsilon = 1 / (self.trials * self.trials)
        return None

class LearningAgentEpsilon3(LearningAgent):
    def reset(self, destination=None, testing=False):
        """ 
        Subclass of `LearningAgent` with epsilon function:
        
            `e ** -(self.alpha * self.trials)`
        """
        self.planner.route_to(destination)
        self.trials += 1
        if testing:
            self.epsilon = 0
            self.alpha = 0
        else:
            e = 2.7182818284
            self.epsilon = e ** (-1 * (self.epsilonA * self.trials))
        return None

class LearningAgentEpsilon4(LearningAgent):
    def reset(self, destination=None, testing=False):
        """ 
        Subclass of `LearningAgent` with epsilon function:
        
            `cos(a * trials)`
        """
        self.planner.route_to(destination)
        self.trials += 1
        if testing:
            self.epsilon = 0
            self.alpha = 0
        else:
            self.epsilon = math.cos(self.epsilonA * self.trials)
        return None

def run():
    """ Driving function for running the simulation. 
        Press ESC to close the simulation, or [SPACE] to pause the simulation. """

    ##############
    # Create the environment
    # Flags:
    #   verbose     - set to True to display additional output from the simulation
    #   num_dummies - discrete number of dummy agents in the environment, default is 100
    #   grid_size   - discrete number of intersections (columns, rows), default is (8, 6)
    env = Environment(verbose=VERBOSE)
    
    ##############
    # Create the driving agent
    # Flags:
    #   learning   - set to True to force the driving agent to use Q-learning
    #    * epsilon - continuous value for the exploration factor, default is 1
    #    * alpha   - continuous value for the learning rate, default is 0.5
    agent = env.create_agent(LearningAgent, learning=True, alpha=DEFAULT_ALPHA)
    
    ##############
    # Follow the driving agent
    # Flags:
    #   enforce_deadline - set to True to enforce a deadline metric
    env.set_primary_agent(agent, enforce_deadline=True)

    ##############
    # Create the simulation
    # Flags:
    #   update_delay - continuous time (in seconds) between actions, default is 2.0 seconds
    #   display      - set to False to disable the GUI if PyGame is enabled
    #   log_metrics  - set to True to log trial and simulation results to /logs
    #   optimized    - set to True to change the default log file name
    sim = Simulator(env, update_delay=0.01, log_metrics=True, optimized=True, display=DISPLAY)
    
    ##############
    # Run the simulator
    # Flags:
    #   tolerance  - epsilon tolerance before beginning testing, default is 0.05 
    #   n_test     - discrete number of testing trials to perform, default is 0
    sim.run(n_test=10)

def calculate_safety(data):
	""" Calculates the safety rating of the smartcab during testing. """

	good_ratio = data['good_actions'].sum() * 1.0 / \
	(data['initial_deadline'] - data['final_deadline']).sum()

	if good_ratio == 1: # Perfect driving
		return ("A+", "green")
	else: # Imperfect driving
		if data['actions'].apply(lambda x: ast.literal_eval(x)[4]).sum() > 0: # Major accident
			return ("F", "red")
		elif data['actions'].apply(lambda x: ast.literal_eval(x)[3]).sum() > 0: # Minor accident
			return ("D", "#EEC700")
		elif data['actions'].apply(lambda x: ast.literal_eval(x)[2]).sum() > 0: # Major violation
			return ("C", "#EEC700")
		else: # Minor violation
			minor = data['actions'].apply(lambda x: ast.literal_eval(x)[1]).sum()
			if minor >= len(data)/2: # Minor violation in at least half of the trials
				return ("B", "green")
			else:
				return ("A", "green")


def calculate_reliability(data):
	""" Calculates the reliability rating of the smartcab during testing. """

	success_ratio = data['success'].sum() * 1.0 / len(data)

	if success_ratio == 1: # Always meets deadline
		return ("A+", "green")
	else:
		if success_ratio >= 0.90:
			return ("A", "green")
		elif success_ratio >= 0.80:
			return ("B", "green")
		elif success_ratio >= 0.70:
			return ("C", "#EEC700")
		elif success_ratio >= 0.60:
			return ("D", "#EEC700")
		else:
			return ("F", "red")


def record_results(grade_log, loop, epsilon, epsilonA, alpha):
    """
    Write results of simulation to grade log file.
    """
    csv = "sim_improved-learning.csv"
    data = pd.read_csv(os.path.join("logs", csv))
    data['good_actions'] = data['actions'].apply(lambda x: ast.literal_eval(x)[0])
    testing_data = data[data['testing']==True]

    reliability, color = calculate_reliability(testing_data)
    safety, color = calculate_safety(testing_data)

    grade_log.write("%s, " % loop)
    grade_log.write("%s, " % epsilon)
    grade_log.write("%s, " % epsilonA)
    grade_log.write("%s, " % alpha)
    grade_log.write("%s, " % safety)
    grade_log.write("%s\n" % reliability)


def run_epsilon(epsilon_function=0, epsilonA=0.5, alpha=0.5):
    """ 
    Run a simulation with customized parameters.
    """
    env = Environment(verbose=VERBOSE)
    learning_agent = LearningAgentEpsilon0
    if epsilon_function == 1:
        learning_agent = LearningAgentEpsilon1
    elif epsilon_function == 2:
        learning_agent = LearningAgentEpsilon2
    elif epsilon_function == 3:
        learning_agent = LearningAgentEpsilon3
    elif epsilon_function == 4:
        learning_agent = LearningAgentEpsilon4
    
    agent = env.create_agent(learning_agent, learning=True, alpha=alpha)
    agent.epsilonA = epsilonA
    env.set_primary_agent(agent, enforce_deadline=True)
    sim = Simulator(env, update_delay=0.01, log_metrics=True, optimized=True, display=DISPLAY)
    sim.run(n_test=10)
    return epsilon_function, agent.epsilonA


def run_control(args):
    """
    Set up and execute multiple simulation runs.
    """
    grade_log = open("logs/grade_log.csv", 'w')
    grade_log.write("Loop, Epsilon, (A), Alpha, Safety, Reliability\n")

    epsilon_function = int(args.epsilon)
    loops = args.loops
    alphas = [0.03, 0.5, 0.95]
    epsilonAs = [0.05, 0.5, 0.95]
    for alpha in alphas:
        for epsilonA in epsilonAs:
            for loop in range(1,loops+1):
                print "Running Loop %i: epsilon=%s (%f) / alpha=%f" % (loop, args.epsilon, epsilonA,  alpha)
                eps, epsA = run_epsilon(epsilon_function, epsilonA, alpha)
                record_results(grade_log, loop, eps, epsA, alpha)


if __name__ == '__main__':
    """
    If no arguments, run as originally designed. Otherwise, run multiple test loops
    with customized parameters.
    """
    args = parse_arguments()
    if args.epsilon == 'empty_string':
        print "Default Simulation"
        run()
    else:
        print "Customized Simulations"
        run_control(args)
